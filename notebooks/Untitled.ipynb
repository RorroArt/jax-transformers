{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset_loaders import WikipediaDataset, WikipediaTokenizerDataset \n",
    "from torch.utils.data import DataLoader\n",
    "from modeling import LSTMLM, Trainer, TrainerConfig\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikipediaTokenizerDataset('./data/wikipedia_articles_en/data.parquet')\n",
    "loader = DataLoader(dataset, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject</th>\n",
       "      <th>previous_blocks</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>? Nycticebus linglom&lt;sep&gt;Taxonomy</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;? Nycticebus linglom was described in 1997 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>? Nycticebus linglom&lt;sep&gt;Description</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;The single known tooth, a third upper molar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>? Nycticebus linglom&lt;sep&gt;Range and ecology</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;Li Mae Long, the collection site of ? N. li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>? Oryzomys pliocaenicus&lt;sep&gt;Discovery and context</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;The only known specimen of ? Oryzomys plioc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>? Oryzomys pliocaenicus&lt;sep&gt;Description</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;? Oryzomys pliocaenicus is known from a sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>? Oryzomys pliocaenicus&lt;sep&gt;Interpretations</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;Hibbard wrote that the condition of the men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Gameplay</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;.hack simulates an MMORPG; players assume t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Plot</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Plot&lt;sep&gt;Setting</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;The .hack games are set in an alternate tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Plot&lt;sep&gt;Characters</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;The main protagonist of .hack is Kite, a ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Plot&lt;sep&gt;Story</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;In .hack//Infection, Kite's friend Orca inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Development</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;Development for .hack began in early 2000 w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Reception</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;By March 2004, sales of the .hack games exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Reception</td>\n",
       "      <td>&lt;sep&gt;By March 2004, sales of the .hack games e...</td>\n",
       "      <td>&lt;s&gt; Dunham gave it an overall rating of 8.4 ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Related media an...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;The .hack video games are part of a multime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Related media an...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;The games' soundtrack, titled .hack//Game M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Related media an...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;.hack//frägment is a multiplayer online gam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Notes</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Notes&lt;sep&gt;Refere...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>.hack (video game series)&lt;sep&gt;Notes&lt;sep&gt;Extern...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;s&gt;dot hack official site&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                            subject  \\\n",
       "0    0                  ? Nycticebus linglom<sep>Taxonomy   \n",
       "1    1               ? Nycticebus linglom<sep>Description   \n",
       "2    2         ? Nycticebus linglom<sep>Range and ecology   \n",
       "3    3  ? Oryzomys pliocaenicus<sep>Discovery and context   \n",
       "4    4            ? Oryzomys pliocaenicus<sep>Description   \n",
       "5    5        ? Oryzomys pliocaenicus<sep>Interpretations   \n",
       "6    6             .hack (video game series)<sep>Gameplay   \n",
       "7    7                 .hack (video game series)<sep>Plot   \n",
       "8    8     .hack (video game series)<sep>Plot<sep>Setting   \n",
       "9    9  .hack (video game series)<sep>Plot<sep>Characters   \n",
       "10  10       .hack (video game series)<sep>Plot<sep>Story   \n",
       "11  11          .hack (video game series)<sep>Development   \n",
       "12  12            .hack (video game series)<sep>Reception   \n",
       "13  13            .hack (video game series)<sep>Reception   \n",
       "14  14  .hack (video game series)<sep>Related media an...   \n",
       "15  15  .hack (video game series)<sep>Related media an...   \n",
       "16  16  .hack (video game series)<sep>Related media an...   \n",
       "17  17                .hack (video game series)<sep>Notes   \n",
       "18  18  .hack (video game series)<sep>Notes<sep>Refere...   \n",
       "19  19  .hack (video game series)<sep>Notes<sep>Extern...   \n",
       "\n",
       "                                      previous_blocks  \\\n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "3                                                       \n",
       "4                                                       \n",
       "5                                                       \n",
       "6                                                       \n",
       "7                                                       \n",
       "8                                                       \n",
       "9                                                       \n",
       "10                                                      \n",
       "11                                                      \n",
       "12                                                      \n",
       "13  <sep>By March 2004, sales of the .hack games e...   \n",
       "14                                                      \n",
       "15                                                      \n",
       "16                                                      \n",
       "17                                                      \n",
       "18                                                      \n",
       "19                                                      \n",
       "\n",
       "                                                 text  \n",
       "0   <s>? Nycticebus linglom was described in 1997 ...  \n",
       "1   <s>The single known tooth, a third upper molar...  \n",
       "2   <s>Li Mae Long, the collection site of ? N. li...  \n",
       "3   <s>The only known specimen of ? Oryzomys plioc...  \n",
       "4   <s>? Oryzomys pliocaenicus is known from a sin...  \n",
       "5   <s>Hibbard wrote that the condition of the men...  \n",
       "6   <s>.hack simulates an MMORPG; players assume t...  \n",
       "7                                             <s></s>  \n",
       "8   <s>The .hack games are set in an alternate tim...  \n",
       "9   <s>The main protagonist of .hack is Kite, a ne...  \n",
       "10  <s>In .hack//Infection, Kite's friend Orca inv...  \n",
       "11  <s>Development for .hack began in early 2000 w...  \n",
       "12  <s>By March 2004, sales of the .hack games exc...  \n",
       "13  <s> Dunham gave it an overall rating of 8.4 ou...  \n",
       "14  <s>The .hack video games are part of a multime...  \n",
       "15  <s>The games' soundtrack, titled .hack//Game M...  \n",
       "16  <s>.hack//frägment is a multiplayer online gam...  \n",
       "17                                            <s></s>  \n",
       "18                                            <s></s>  \n",
       "19                      <s>dot hack official site</s>  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.frame.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658824"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<s>', '</s>', '<unk>', '<pad>', '<sep>']\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.decoders = decoders.ByteLevel()\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=50000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "tokenizer.train_from_iterator(loader, trainer=trainer)\n",
    "tokenizer.enable_padding(pad_token='<pad>', pad_id=4)\n",
    "tokenizer.save(\"./custom_tokenizers/models/testing_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"./custom_tokenizers/models/testing_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrorroart\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.18<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">desert-dew-11</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/rorroart/uncategorized\" target=\"_blank\">https://wandb.ai/rorroart/uncategorized</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/rorroart/uncategorized/runs/2awbfmwp\" target=\"_blank\">https://wandb.ai/rorroart/uncategorized/runs/2awbfmwp</a><br/>\n",
       "                Run data is saved locally in <code>/mnt/d/text-autocompletation/wandb/run-20210211_142804-2awbfmwp</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/82353 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0aadf361eb1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainerConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/d/text-autocompletation/modeling/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/text-autocompletation/modeling/trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0mloss_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tion/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/text-autocompletation/modeling/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, last_hidden, context, targets)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mavg_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_context\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tion/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/text-autocompletation/modeling/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mprev_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_dataset = WikipediaDataset('./data/wikipedia_articles_en/data.parquet')\n",
    "dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "model = LSTMLM(50000, 512, 1024, 2, context_dim=512, residual=True, dropout=0.1)\n",
    "criterion = nn.NLLLoss()\n",
    "trainer = Trainer(model, tokenizer, criterion, train_dataset, None, TrainerConfig(batch_size=8, use_context=True))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from modeling import PlainLSTMLM\n",
    "\n",
    "seed = 1234\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(inputs, tokenizer, device):\n",
    "    encoded_inputs = tokenizer.encode_batch(inputs)\n",
    "    inputs_ids = [encoded_input.ids for encoded_input in encoded_inputs]\n",
    "    input_tensor = torch.LongTensor(inputs_ids).transpose(0,1)\n",
    "    return input_tensor[:-1, :].to(device), input_tensor[1:, :].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
       "         [  543,  2062,   299,  ...,  1892,  6952,  9762],\n",
       "         [14535, 21489,   260,  ...,   281,  8061,    16],\n",
       "         ...,\n",
       "         [    4,     4,   237,  ...,     4,     4,     4],\n",
       "         [    4,     4,  2475,  ...,     4,     4,     4],\n",
       "         [    4,     4,    18,  ...,     4,     4,     4]], device='cuda:0'),\n",
       " tensor([[  543,  2062,   299,  ...,  1892,  6952,  9762],\n",
       "         [14535, 21489,   260,  ...,   281,  8061,    16],\n",
       "         [   16, 42580,    17,  ...,  1273,  1514,  1720],\n",
       "         ...,\n",
       "         [    4,     4,  2475,  ...,     4,     4,     4],\n",
       "         [    4,     4,    18,  ...,     4,     4,     4],\n",
       "         [    4,     4,     1,  ...,     4,     4,     4]], device='cuda:0'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(i, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, criterion, loader):\n",
    "    loss_sum = 0\n",
    "    total_loss = 0\n",
    "    for i, example in enumerate(loader):\n",
    "        x, target = example['input_ids'].transpose(0,1)[:-1], example['input_ids'].transpose(0,1)[1:]\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        hidden = None\n",
    "        loss = 0\n",
    "        topk = torch.ones(1, x.size(1), 1).to(device)\n",
    "        for t in range(0, x.size(0)):\n",
    "            output, hidden = model(x[t,:].unsqueeze(0),\n",
    "                               hidden)\n",
    "            l = criterion(output.squeeze(0), target[t, :])\n",
    "            loss += l  \n",
    "            topk_v, topk_i = output.topk(1, dim=2)\n",
    "            topk = torch.cat((topk,topk_i), dim=0)\n",
    "        loss_sum += loss/x.size(0)\n",
    "        total_loss += 1\n",
    "    #calculate metrics\n",
    "    final_loss = loss_sum / total_loss\n",
    "    perplexity = torch.exp(final_loss)\n",
    "    \n",
    "    #detokenize some sentence\n",
    "    batch_example = random.randint(0,x.size(1)-1)\n",
    "    input_sentence = detokenize(x.transpose(0,1)[0,:])\n",
    "    output_sentence = detokenize(topk.transpose(0,1)[0, :])\n",
    "    \n",
    "    #print everything\n",
    "    validation_info = \"\"\"-----------------------------------------------------\n",
    "    Validation:\n",
    "    loss: %.4f, perplexity: %.4f\n",
    "    input sentence: %s\n",
    "    output sentence: %s\n",
    "    ------------------------------------------------\n",
    "    \"\"\" % (final_loss, perplexity, input_sentence, output_sentence)\n",
    "    print(validation_info)\n",
    "    return final_loss, perplexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, criterion, optimizer, inputs):\n",
    "    x, target = inputs\n",
    "    x, target = x.to(device), target.to(device)\n",
    "    #loss_lengths = lengths.to(device)\n",
    "    hidden = None\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    output = x[0:1,:]\n",
    "    for t in range(0, x.size(0)):\n",
    "        output, hidden = model(x[t,:].unsqueeze(0),\n",
    "                               hidden)\n",
    "        l = criterion(output.squeeze(0), target[t, :])\n",
    "        loss += l\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss/x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, criterion, optimizer, loader, epochs, print_every=10, save_every=1000):\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for i, batch in enumerate(loader):\n",
    "            input_batch = batch[2]\n",
    "            inputs = get_batch(input_batch, tokenizer, device)\n",
    "            #lengths = example['lengths'].transpose(0,1)\n",
    "            output, loss = train_step(model, criterion, optimizer, inputs)\n",
    "            if i % print_every == 0:\n",
    "                print('epoch: %.d, iter: %.d, loss: %.4f' % \n",
    "                     (epoch, i, loss))\n",
    "            if i % save_every == 0:\n",
    "                torch.save({\n",
    "                    'iteration': i,\n",
    "                    'epoch': epoch,\n",
    "                    'model': model.state_dict()\n",
    "                }, './models/small-test/{}_{}.tar'.format(epoch,i))\n",
    "def one_batch(model, tokenizer, criterion, optimizer, batch, iters):\n",
    "    inputs = get_batch(batch[2], tokenizer, device)\n",
    "    #lengths = batch['lengths'].transpose(0,1)\n",
    "    for i in range(iters):\n",
    "        output, loss = train_step(model, criterion, optimizer, inputs)\n",
    "        if i % 1 == 0:\n",
    "            print('iter: %.d, loss: %.4f' % \n",
    "                 (i, loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 2.0796\n",
      "iter: 1, loss: 2.0471\n",
      "iter: 2, loss: 1.9400\n",
      "iter: 3, loss: 1.8202\n",
      "iter: 4, loss: 1.7865\n",
      "iter: 5, loss: 1.7377\n",
      "iter: 6, loss: 1.6900\n",
      "iter: 7, loss: 1.6795\n",
      "iter: 8, loss: 1.6270\n",
      "iter: 9, loss: 1.6042\n",
      "iter: 10, loss: 1.5851\n",
      "iter: 11, loss: 1.5675\n",
      "iter: 12, loss: 1.5511\n",
      "iter: 13, loss: 1.5364\n",
      "iter: 14, loss: 1.5270\n",
      "iter: 15, loss: 1.5207\n",
      "iter: 16, loss: 1.5150\n",
      "iter: 17, loss: 1.5080\n",
      "iter: 18, loss: 1.5053\n",
      "iter: 19, loss: 1.5023\n",
      "iter: 20, loss: 1.4992\n",
      "iter: 21, loss: 1.4968\n",
      "iter: 22, loss: 1.4940\n",
      "iter: 23, loss: 1.4913\n",
      "iter: 24, loss: 1.4889\n",
      "iter: 25, loss: 1.4864\n",
      "iter: 26, loss: 1.4835\n",
      "iter: 27, loss: 1.4804\n",
      "iter: 28, loss: 1.4770\n",
      "iter: 29, loss: 1.4731\n",
      "iter: 30, loss: 1.4673\n",
      "iter: 31, loss: 1.4673\n",
      "iter: 32, loss: 1.4646\n",
      "iter: 33, loss: 1.4604\n",
      "iter: 34, loss: 1.4563\n",
      "iter: 35, loss: 1.4526\n",
      "iter: 36, loss: 1.4467\n",
      "iter: 37, loss: 1.4382\n",
      "iter: 38, loss: 1.4304\n",
      "iter: 39, loss: 1.4246\n",
      "iter: 40, loss: 1.4115\n",
      "iter: 41, loss: 1.4014\n",
      "iter: 42, loss: 1.3919\n",
      "iter: 43, loss: 1.3797\n",
      "iter: 44, loss: 1.3678\n",
      "iter: 45, loss: 1.3553\n",
      "iter: 46, loss: 1.3437\n",
      "iter: 47, loss: 1.3319\n",
      "iter: 48, loss: 1.3235\n",
      "iter: 49, loss: 1.3017\n",
      "iter: 50, loss: 1.2900\n",
      "iter: 51, loss: 1.2814\n",
      "iter: 52, loss: 1.2599\n",
      "iter: 53, loss: 1.2530\n",
      "iter: 54, loss: 1.2347\n",
      "iter: 55, loss: 1.2295\n",
      "iter: 56, loss: 1.2164\n",
      "iter: 57, loss: 1.2030\n",
      "iter: 58, loss: 1.1898\n",
      "iter: 59, loss: 1.1784\n",
      "iter: 60, loss: 1.1628\n",
      "iter: 61, loss: 1.1532\n",
      "iter: 62, loss: 1.1422\n",
      "iter: 63, loss: 1.1311\n",
      "iter: 64, loss: 1.1208\n",
      "iter: 65, loss: 1.1062\n",
      "iter: 66, loss: 1.0950\n",
      "iter: 67, loss: 1.0857\n",
      "iter: 68, loss: 1.0803\n",
      "iter: 69, loss: 1.0693\n",
      "iter: 70, loss: 1.0572\n",
      "iter: 71, loss: 1.0465\n",
      "iter: 72, loss: 1.0418\n",
      "iter: 73, loss: 1.0356\n",
      "iter: 74, loss: 1.0286\n",
      "iter: 75, loss: 1.0159\n",
      "iter: 76, loss: 1.0098\n",
      "iter: 77, loss: 1.0043\n",
      "iter: 78, loss: 1.0074\n",
      "iter: 79, loss: 1.0237\n",
      "iter: 80, loss: 1.0553\n",
      "iter: 81, loss: 1.0562\n",
      "iter: 82, loss: 1.0428\n",
      "iter: 83, loss: 1.0510\n",
      "iter: 84, loss: 1.0435\n",
      "iter: 85, loss: 1.0373\n",
      "iter: 86, loss: 1.0273\n",
      "iter: 87, loss: 1.0199\n",
      "iter: 88, loss: 1.0134\n",
      "iter: 89, loss: 1.0027\n",
      "iter: 90, loss: 0.9954\n",
      "iter: 91, loss: 0.9882\n",
      "iter: 92, loss: 0.9821\n",
      "iter: 93, loss: 0.9756\n",
      "iter: 94, loss: 0.9693\n",
      "iter: 95, loss: 0.9636\n",
      "iter: 96, loss: 0.9582\n",
      "iter: 97, loss: 0.9533\n",
      "iter: 98, loss: 0.9484\n",
      "iter: 99, loss: 0.9445\n",
      "iter: 100, loss: 0.9391\n",
      "iter: 101, loss: 0.9349\n",
      "iter: 102, loss: 0.9313\n",
      "iter: 103, loss: 0.9284\n",
      "iter: 104, loss: 0.9250\n",
      "iter: 105, loss: 0.9204\n",
      "iter: 106, loss: 0.9162\n",
      "iter: 107, loss: 0.9122\n",
      "iter: 108, loss: 0.9094\n",
      "iter: 109, loss: 0.9071\n",
      "iter: 110, loss: 0.9045\n",
      "iter: 111, loss: 0.9022\n",
      "iter: 112, loss: 0.8984\n",
      "iter: 113, loss: 0.8948\n",
      "iter: 114, loss: 0.8919\n",
      "iter: 115, loss: 0.8892\n",
      "iter: 116, loss: 0.8873\n",
      "iter: 117, loss: 0.8847\n",
      "iter: 118, loss: 0.8822\n",
      "iter: 119, loss: 0.8800\n",
      "iter: 120, loss: 0.8780\n",
      "iter: 121, loss: 0.8762\n",
      "iter: 122, loss: 0.8737\n",
      "iter: 123, loss: 0.8722\n",
      "iter: 124, loss: 0.8713\n",
      "iter: 125, loss: 0.8707\n",
      "iter: 126, loss: 0.8722\n",
      "iter: 127, loss: 0.8743\n",
      "iter: 128, loss: 0.8692\n",
      "iter: 129, loss: 0.8633\n",
      "iter: 130, loss: 0.8664\n",
      "iter: 131, loss: 0.8625\n",
      "iter: 132, loss: 0.8610\n",
      "iter: 133, loss: 0.8612\n",
      "iter: 134, loss: 0.8568\n",
      "iter: 135, loss: 0.8585\n",
      "iter: 136, loss: 0.8539\n",
      "iter: 137, loss: 0.8548\n",
      "iter: 138, loss: 0.8515\n",
      "iter: 139, loss: 0.8517\n",
      "iter: 140, loss: 0.8492\n",
      "iter: 141, loss: 0.8500\n",
      "iter: 142, loss: 0.8477\n",
      "iter: 143, loss: 0.8470\n",
      "iter: 144, loss: 0.8463\n",
      "iter: 145, loss: 0.8449\n",
      "iter: 146, loss: 0.8437\n",
      "iter: 147, loss: 0.8432\n",
      "iter: 148, loss: 0.8421\n",
      "iter: 149, loss: 0.8413\n",
      "iter: 150, loss: 0.8403\n",
      "iter: 151, loss: 0.8391\n",
      "iter: 152, loss: 0.8388\n",
      "iter: 153, loss: 0.8374\n",
      "iter: 154, loss: 0.8373\n",
      "iter: 155, loss: 0.8355\n",
      "iter: 156, loss: 0.8358\n",
      "iter: 157, loss: 0.8345\n",
      "iter: 158, loss: 0.8337\n",
      "iter: 159, loss: 0.8327\n",
      "iter: 160, loss: 0.8323\n",
      "iter: 161, loss: 0.8321\n",
      "iter: 162, loss: 0.8310\n",
      "iter: 163, loss: 0.8305\n",
      "iter: 164, loss: 0.8296\n",
      "iter: 165, loss: 0.8292\n",
      "iter: 166, loss: 0.8282\n",
      "iter: 167, loss: 0.8280\n",
      "iter: 168, loss: 0.8272\n",
      "iter: 169, loss: 0.8269\n",
      "iter: 170, loss: 0.8261\n",
      "iter: 171, loss: 0.8260\n",
      "iter: 172, loss: 0.8256\n",
      "iter: 173, loss: 0.8251\n",
      "iter: 174, loss: 0.8244\n",
      "iter: 175, loss: 0.8243\n",
      "iter: 176, loss: 0.8237\n",
      "iter: 177, loss: 0.8230\n",
      "iter: 178, loss: 0.8228\n",
      "iter: 179, loss: 0.8225\n",
      "iter: 180, loss: 0.8221\n",
      "iter: 181, loss: 0.8215\n",
      "iter: 182, loss: 0.8214\n",
      "iter: 183, loss: 0.8209\n",
      "iter: 184, loss: 0.8203\n",
      "iter: 185, loss: 0.8201\n",
      "iter: 186, loss: 0.8198\n",
      "iter: 187, loss: 0.8193\n",
      "iter: 188, loss: 0.8190\n",
      "iter: 189, loss: 0.8183\n",
      "iter: 190, loss: 0.8183\n",
      "iter: 191, loss: 0.8180\n",
      "iter: 192, loss: 0.8175\n",
      "iter: 193, loss: 0.8174\n",
      "iter: 194, loss: 0.8170\n",
      "iter: 195, loss: 0.8166\n",
      "iter: 196, loss: 0.8162\n",
      "iter: 197, loss: 0.8161\n",
      "iter: 198, loss: 0.8160\n",
      "iter: 199, loss: 0.8155\n",
      "iter: 200, loss: 0.8153\n",
      "iter: 201, loss: 0.8156\n",
      "iter: 202, loss: 0.8148\n",
      "iter: 203, loss: 0.8146\n",
      "iter: 204, loss: 0.8146\n",
      "iter: 205, loss: 0.8147\n",
      "iter: 206, loss: 0.8141\n",
      "iter: 207, loss: 0.8142\n",
      "iter: 208, loss: 0.8140\n",
      "iter: 209, loss: 0.8138\n",
      "iter: 210, loss: 0.8132\n",
      "iter: 211, loss: 0.8131\n",
      "iter: 212, loss: 0.8126\n",
      "iter: 213, loss: 0.8125\n",
      "iter: 214, loss: 0.8133\n",
      "iter: 215, loss: 0.8122\n",
      "iter: 216, loss: 0.8120\n",
      "iter: 217, loss: 0.8121\n",
      "iter: 218, loss: 0.8122\n",
      "iter: 219, loss: 0.8120\n",
      "iter: 220, loss: 0.8116\n",
      "iter: 221, loss: 0.8113\n",
      "iter: 222, loss: 0.8111\n",
      "iter: 223, loss: 0.8108\n",
      "iter: 224, loss: 0.8106\n",
      "iter: 225, loss: 0.8103\n",
      "iter: 226, loss: 0.8101\n",
      "iter: 227, loss: 0.8099\n",
      "iter: 228, loss: 0.8096\n",
      "iter: 229, loss: 0.8096\n",
      "iter: 230, loss: 0.8093\n",
      "iter: 231, loss: 0.8094\n",
      "iter: 232, loss: 0.8094\n",
      "iter: 233, loss: 0.8088\n",
      "iter: 234, loss: 0.8087\n",
      "iter: 235, loss: 0.8087\n",
      "iter: 236, loss: 0.8084\n",
      "iter: 237, loss: 0.8083\n",
      "iter: 238, loss: 0.8084\n",
      "iter: 239, loss: 0.8084\n",
      "iter: 240, loss: 0.8080\n",
      "iter: 241, loss: 0.8077\n",
      "iter: 242, loss: 0.8077\n",
      "iter: 243, loss: 0.8075\n",
      "iter: 244, loss: 0.8072\n",
      "iter: 245, loss: 0.8073\n",
      "iter: 246, loss: 0.8072\n",
      "iter: 247, loss: 0.8071\n",
      "iter: 248, loss: 0.8070\n",
      "iter: 249, loss: 0.8067\n",
      "iter: 250, loss: 0.8068\n",
      "iter: 251, loss: 0.8068\n",
      "iter: 252, loss: 0.8064\n",
      "iter: 253, loss: 0.8067\n",
      "iter: 254, loss: 0.8064\n",
      "iter: 255, loss: 0.8064\n",
      "iter: 256, loss: 0.8065\n",
      "iter: 257, loss: 0.8062\n",
      "iter: 258, loss: 0.8060\n",
      "iter: 259, loss: 0.8059\n",
      "iter: 260, loss: 0.8060\n",
      "iter: 261, loss: 0.8056\n",
      "iter: 262, loss: 0.8058\n",
      "iter: 263, loss: 0.8053\n",
      "iter: 264, loss: 0.8052\n",
      "iter: 265, loss: 0.8053\n",
      "iter: 266, loss: 0.8049\n",
      "iter: 267, loss: 0.8050\n",
      "iter: 268, loss: 0.8052\n",
      "iter: 269, loss: 0.8046\n",
      "iter: 270, loss: 0.8047\n",
      "iter: 271, loss: 0.8047\n",
      "iter: 272, loss: 0.8046\n",
      "iter: 273, loss: 0.8044\n",
      "iter: 274, loss: 0.8046\n",
      "iter: 275, loss: 0.8044\n",
      "iter: 276, loss: 0.8039\n",
      "iter: 277, loss: 0.8041\n",
      "iter: 278, loss: 0.8039\n",
      "iter: 279, loss: 0.8039\n",
      "iter: 280, loss: 0.8040\n",
      "iter: 281, loss: 0.8036\n",
      "iter: 282, loss: 0.8036\n",
      "iter: 283, loss: 0.8035\n",
      "iter: 284, loss: 0.8034\n",
      "iter: 285, loss: 0.8035\n",
      "iter: 286, loss: 0.8034\n",
      "iter: 287, loss: 0.8033\n",
      "iter: 288, loss: 0.8032\n",
      "iter: 289, loss: 0.8030\n",
      "iter: 290, loss: 0.8030\n",
      "iter: 291, loss: 0.8031\n",
      "iter: 292, loss: 0.8030\n",
      "iter: 293, loss: 0.8030\n",
      "iter: 294, loss: 0.8025\n",
      "iter: 295, loss: 0.8026\n",
      "iter: 296, loss: 0.8029\n",
      "iter: 297, loss: 0.8027\n",
      "iter: 298, loss: 0.8027\n",
      "iter: 299, loss: 0.8024\n",
      "iter: 300, loss: 0.8023\n",
      "iter: 301, loss: 0.8024\n",
      "iter: 302, loss: 0.8023\n",
      "iter: 303, loss: 0.8021\n",
      "iter: 304, loss: 0.8021\n",
      "iter: 305, loss: 0.8021\n",
      "iter: 306, loss: 0.8021\n",
      "iter: 307, loss: 0.8020\n",
      "iter: 308, loss: 0.8017\n",
      "iter: 309, loss: 0.8017\n",
      "iter: 310, loss: 0.8018\n",
      "iter: 311, loss: 0.8017\n",
      "iter: 312, loss: 0.8017\n",
      "iter: 313, loss: 0.8017\n",
      "iter: 314, loss: 0.8016\n",
      "iter: 315, loss: 0.8014\n",
      "iter: 316, loss: 0.8014\n",
      "iter: 317, loss: 0.8012\n",
      "iter: 318, loss: 0.8012\n",
      "iter: 319, loss: 0.8012\n",
      "iter: 320, loss: 0.8010\n",
      "iter: 321, loss: 0.8012\n",
      "iter: 322, loss: 0.8009\n",
      "iter: 323, loss: 0.8008\n",
      "iter: 324, loss: 0.8009\n",
      "iter: 325, loss: 0.8009\n",
      "iter: 326, loss: 0.8008\n",
      "iter: 327, loss: 0.8007\n",
      "iter: 328, loss: 0.8007\n",
      "iter: 329, loss: 0.8007\n",
      "iter: 330, loss: 0.8006\n",
      "iter: 331, loss: 0.8006\n",
      "iter: 332, loss: 0.8004\n",
      "iter: 333, loss: 0.8005\n",
      "iter: 334, loss: 0.8005\n",
      "iter: 335, loss: 0.8003\n",
      "iter: 336, loss: 0.8005\n",
      "iter: 337, loss: 0.8004\n",
      "iter: 338, loss: 0.8002\n",
      "iter: 339, loss: 0.8001\n",
      "iter: 340, loss: 0.8001\n",
      "iter: 341, loss: 0.8001\n",
      "iter: 342, loss: 0.7999\n",
      "iter: 343, loss: 0.7999\n",
      "iter: 344, loss: 0.7999\n",
      "iter: 345, loss: 0.7999\n",
      "iter: 346, loss: 0.7998\n",
      "iter: 347, loss: 0.7996\n",
      "iter: 348, loss: 0.7998\n",
      "iter: 349, loss: 0.7998\n",
      "iter: 350, loss: 0.7998\n",
      "iter: 351, loss: 0.7996\n",
      "iter: 352, loss: 0.7998\n",
      "iter: 353, loss: 0.7996\n",
      "iter: 354, loss: 0.7995\n",
      "iter: 355, loss: 0.7995\n",
      "iter: 356, loss: 0.7994\n",
      "iter: 357, loss: 0.7993\n",
      "iter: 358, loss: 0.7994\n",
      "iter: 359, loss: 0.7996\n",
      "iter: 360, loss: 0.7992\n",
      "iter: 361, loss: 0.7992\n",
      "iter: 362, loss: 0.7992\n",
      "iter: 363, loss: 0.7991\n",
      "iter: 364, loss: 0.7990\n",
      "iter: 365, loss: 0.7992\n",
      "iter: 366, loss: 0.7992\n",
      "iter: 367, loss: 0.7988\n",
      "iter: 368, loss: 0.7989\n",
      "iter: 369, loss: 0.7989\n",
      "iter: 370, loss: 0.7990\n",
      "iter: 371, loss: 0.7988\n",
      "iter: 372, loss: 0.7988\n",
      "iter: 373, loss: 0.7986\n",
      "iter: 374, loss: 0.7987\n",
      "iter: 375, loss: 0.7987\n",
      "iter: 376, loss: 0.7986\n",
      "iter: 377, loss: 0.7986\n",
      "iter: 378, loss: 0.7985\n",
      "iter: 379, loss: 0.7986\n",
      "iter: 380, loss: 0.7985\n",
      "iter: 381, loss: 0.7987\n",
      "iter: 382, loss: 0.7984\n",
      "iter: 383, loss: 0.7985\n",
      "iter: 384, loss: 0.7982\n",
      "iter: 385, loss: 0.7982\n",
      "iter: 386, loss: 0.7981\n",
      "iter: 387, loss: 0.7983\n",
      "iter: 388, loss: 0.7981\n",
      "iter: 389, loss: 0.7983\n",
      "iter: 390, loss: 0.7982\n",
      "iter: 391, loss: 0.7981\n",
      "iter: 392, loss: 0.7980\n",
      "iter: 393, loss: 0.7980\n",
      "iter: 394, loss: 0.7980\n",
      "iter: 395, loss: 0.7979\n",
      "iter: 396, loss: 0.7980\n",
      "iter: 397, loss: 0.7978\n",
      "iter: 398, loss: 0.7979\n",
      "iter: 399, loss: 0.7978\n",
      "iter: 400, loss: 0.7977\n",
      "iter: 401, loss: 0.7978\n",
      "iter: 402, loss: 0.7977\n",
      "iter: 403, loss: 0.7977\n",
      "iter: 404, loss: 0.7977\n",
      "iter: 405, loss: 0.7978\n",
      "iter: 406, loss: 0.7977\n",
      "iter: 407, loss: 0.7977\n",
      "iter: 408, loss: 0.7977\n",
      "iter: 409, loss: 0.7978\n",
      "iter: 410, loss: 0.7974\n",
      "iter: 411, loss: 0.7976\n",
      "iter: 412, loss: 0.7975\n",
      "iter: 413, loss: 0.7975\n",
      "iter: 414, loss: 0.7976\n",
      "iter: 415, loss: 0.7977\n",
      "iter: 416, loss: 0.7974\n",
      "iter: 417, loss: 0.7977\n",
      "iter: 418, loss: 0.7986\n",
      "iter: 419, loss: 0.7977\n",
      "iter: 420, loss: 0.7976\n",
      "iter: 421, loss: 0.7977\n",
      "iter: 422, loss: 0.7975\n",
      "iter: 423, loss: 0.7975\n",
      "iter: 424, loss: 0.7974\n",
      "iter: 425, loss: 0.7976\n",
      "iter: 426, loss: 0.7974\n",
      "iter: 427, loss: 0.7974\n",
      "iter: 428, loss: 0.7973\n",
      "iter: 429, loss: 0.7973\n",
      "iter: 430, loss: 0.7973\n",
      "iter: 431, loss: 0.7972\n",
      "iter: 432, loss: 0.7971\n",
      "iter: 433, loss: 0.7972\n",
      "iter: 434, loss: 0.7971\n",
      "iter: 435, loss: 0.7971\n",
      "iter: 436, loss: 0.7971\n",
      "iter: 437, loss: 0.7970\n",
      "iter: 438, loss: 0.7970\n",
      "iter: 439, loss: 0.7970\n",
      "iter: 440, loss: 0.7970\n",
      "iter: 441, loss: 0.7970\n",
      "iter: 442, loss: 0.7969\n",
      "iter: 443, loss: 0.7969\n",
      "iter: 444, loss: 0.7968\n",
      "iter: 445, loss: 0.7967\n",
      "iter: 446, loss: 0.7969\n",
      "iter: 447, loss: 0.7968\n",
      "iter: 448, loss: 0.7967\n",
      "iter: 449, loss: 0.7965\n",
      "iter: 450, loss: 0.7967\n",
      "iter: 451, loss: 0.7966\n",
      "iter: 452, loss: 0.7965\n",
      "iter: 453, loss: 0.7966\n",
      "iter: 454, loss: 0.7965\n",
      "iter: 455, loss: 0.7965\n",
      "iter: 456, loss: 0.7965\n",
      "iter: 457, loss: 0.7966\n",
      "iter: 458, loss: 0.7965\n",
      "iter: 459, loss: 0.7965\n",
      "iter: 460, loss: 0.7965\n",
      "iter: 461, loss: 0.7964\n",
      "iter: 462, loss: 0.7964\n",
      "iter: 463, loss: 0.7965\n",
      "iter: 464, loss: 0.7963\n",
      "iter: 465, loss: 0.7964\n",
      "iter: 466, loss: 0.7962\n",
      "iter: 467, loss: 0.7965\n",
      "iter: 468, loss: 0.7963\n",
      "iter: 469, loss: 0.7964\n",
      "iter: 470, loss: 0.7963\n",
      "iter: 471, loss: 0.7962\n",
      "iter: 472, loss: 0.7962\n",
      "iter: 473, loss: 0.7962\n",
      "iter: 474, loss: 0.7962\n",
      "iter: 475, loss: 0.7963\n",
      "iter: 476, loss: 0.7963\n",
      "iter: 477, loss: 0.7962\n",
      "iter: 478, loss: 0.7963\n",
      "iter: 479, loss: 0.7963\n",
      "iter: 480, loss: 0.7962\n",
      "iter: 481, loss: 0.7961\n",
      "iter: 482, loss: 0.7961\n",
      "iter: 483, loss: 0.7961\n",
      "iter: 484, loss: 0.7962\n",
      "iter: 485, loss: 0.7961\n",
      "iter: 486, loss: 0.7962\n",
      "iter: 487, loss: 0.7960\n",
      "iter: 488, loss: 0.7960\n",
      "iter: 489, loss: 0.7960\n",
      "iter: 490, loss: 0.7960\n",
      "iter: 491, loss: 0.7960\n",
      "iter: 492, loss: 0.7962\n",
      "iter: 493, loss: 0.7959\n",
      "iter: 494, loss: 0.7960\n",
      "iter: 495, loss: 0.7959\n",
      "iter: 496, loss: 0.7961\n",
      "iter: 497, loss: 0.7959\n",
      "iter: 498, loss: 0.7958\n",
      "iter: 499, loss: 0.7959\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-4\n",
    "\n",
    "model = PlainLSTMLM(50000, 512, 1024, 4, 0.1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def maskNLLLoss(output, target, lengths):\n",
    "    print(lengths)\n",
    "    print(target)\n",
    "    total = lengths.sum()\n",
    "    cross_entropy = -torch.log(torch.gather(output, 1, target.view(-1,1)).squeeze(1))\n",
    "    print(cross_entropy)\n",
    "    loss = (cross_entropy*lengths).sum() / total\n",
    "    loss = loss.to(device)\n",
    "    return loss\n",
    "batch = next(iter(dataloader))\n",
    "one_batch(model, tokenizer, criterion, optimizer, batch, 500) # overfit one batch\n",
    "#train(model, criterion, optimizer, dataloader, 2, 10) # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-f2de6a36cdb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 output, hidden = model(x,\n\u001b[0m\u001b[1;32m     13\u001b[0m                                    hidden)\n\u001b[1;32m     14\u001b[0m                 \u001b[0mtopk_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tion/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/text-autocompletation/modeling/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, last_hidden)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tion/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tion/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    582\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, example in enumerate(loader):\n",
    "\n",
    "            hidden = None\n",
    "            loss = 0\n",
    "            topk = torch.ones(1, x.size(1), 1).to(device)\n",
    "            token = ['<s>India']\n",
    "            sentence = ''\n",
    "            for t in range(0, 5):\n",
    "                x, target = get_batch(token, tokenizer, device)\n",
    "                x = x[0,:].unsqueeze(0)\n",
    "                output, hidden = model(x,\n",
    "                                   hidden)\n",
    "                topk_v, topk_i = output.topk(1, dim=2)\n",
    "                topk = torch.cat((topk,topk_i), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0]]], device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(False)\n",
    "output = model(torch.tensor([[0]], device=device))[0]\n",
    "topk_v, topk_i = output.topk(2, dim=2)\n",
    "topk_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.]]], device='cuda:0', grad_fn=<TopkBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, target = get_batch(['<s>India'], tokenizer, device)\n",
    "x = tokenizer.encode('<s>')\n",
    "out, hidden = model(torch.tensor([x.ids], device=device), None)\n",
    "topk_v, topk_i = out.topk(1, dim=2)\n",
    "topk_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 and 5\n",
      "2 and 6\n",
      "3 and 7\n",
      "4 and 8\n"
     ]
    }
   ],
   "source": [
    "l1 = [1,2,3,4]\n",
    "l2 = [5,6,7,8]\n",
    "for a, b in zip(l1, l2):\n",
    "    print('{} and {}'.format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0]]], device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer.encode('India')\n",
    "out, hidden = model(torch.tensor([x.ids], device=device), hidden)\n",
    "topk_v, topk_i = out.topk(1, dim=2)\n",
    "topk_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-79106250600d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./models/test_overfit.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model, './models/test_overfit.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
