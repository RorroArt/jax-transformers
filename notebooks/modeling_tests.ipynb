{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Sampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset large_spanish_corpus (/home/rodrigo/.cache/huggingface/datasets/large_spanish_corpus/all_wikis/1.1.0/f71a935424f00d2356deff29366f4b499ce0e22957180f5420da5acbbb50e2ec)\n",
      "Loading cached shuffled indices for dataset at /home/rodrigo/.cache/huggingface/datasets/large_spanish_corpus/all_wikis/1.1.0/f71a935424f00d2356deff29366f4b499ce0e22957180f5420da5acbbb50e2ec/cache-b90856c99c6bfc66.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf5e40f62cb491cb3638498c676b30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb26c7eda04d4131a3d7e1b19fa534f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_seq(e):\n",
    "    seq = e['text']\n",
    "    if len(seq) < 499 and len(seq) > 10:\n",
    "        return True\n",
    "    return False\n",
    "dataset = load_dataset(\"large_spanish_corpus\", \"all_wikis\", split='train').shuffle(seed=seed)\n",
    "dataset = dataset.train_test_split(train_size=100000, test_size=1000)\n",
    "dataset = dataset.filter(filter_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = ['<p>', '<s>', '</s>', '<uk>']\n",
    "vocab = string.ascii_letters + '1234567890áéíóú' + '.,:;-<>\"\" '\n",
    "vocab = special_characters + list(vocab)\n",
    "max_length = 500\n",
    "\n",
    "def tokenize(seq, pad=False, max_length=None):\n",
    "    tokens = [1] #sos\n",
    "    for char in seq:\n",
    "        try:\n",
    "            index = vocab.index(char)\n",
    "            tokens.append(index)\n",
    "        except:\n",
    "            tokens.append(3) #ukn\n",
    "    tokens.append(2) #eos\n",
    "    lengths = [1 for _ in tokens]\n",
    "    if pad:\n",
    "        pads = max_length - len(tokens)\n",
    "        tokens += [0 for _ in range(pads)]\n",
    "        lengths += [0 for _ in range(pads)]\n",
    "    return {'input_ids': torch.tensor(tokens)}\n",
    "\n",
    "def encode(e):\n",
    "    return tokenize(e['text'], pad=True, max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd117a1d24e74e04a54661ae5bff766b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=91239.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1a9693b2b84c098392d599934af2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=901.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "batch_size = 32\n",
    "dataset = dataset.map(encode)\n",
    "dataset.set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigo/tion/lib/python3.8/site-packages/datasets/arrow_dataset.py:851: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1, 30, 15,  ...,  0,  0,  0],\n",
       "         [ 1, 44,  6,  ...,  0,  0,  0],\n",
       "         [ 1, 48, 12,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 1, 34, 17,  ...,  0,  0,  0],\n",
       "         [ 1, 30, 80,  ...,  0,  0,  0],\n",
       "         [ 1, 34, 15,  ...,  0,  0,  0]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset['train'], \n",
    "                                         batch_size=batch_size, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x, last_hidden=None):\n",
    "        x = self.embeddings(x)\n",
    "        #x = nn.utils.rnn.pack_padded_sequence(x, lengths,\n",
    "                                            # enforce_sorted=False)\n",
    "        out, h = self.gru(x, last_hidden)\n",
    "        #out, _ = nn.utils.rnn.pad_packed_sequence(out)\n",
    "        out = self.out(out)\n",
    "        out = self.softmax(out)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LMGRU(vocab_size, 512, 128, 8, 0.1)\n",
    "o, h = lstm(batch['input_ids'].transpose(0,1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7122d51a5fd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, criterion, loader):\n",
    "    loss_sum = 0\n",
    "    total_loss = 0\n",
    "    for i, example in enumerate(loader):\n",
    "        x, target = example['input_ids'].transpose(0,1)[:-1], example['input_ids'].transpose(0,1)[1:]\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        hidden = None\n",
    "        loss = 0\n",
    "        topk = torch.ones(1, x.size(1), 1).to(device)\n",
    "        for t in range(0, x.size(0)):\n",
    "            output, hidden = model(x[t,:].unsqueeze(0),\n",
    "                               hidden)\n",
    "            l = criterion(output.squeeze(0), target[t, :])\n",
    "            loss += l  \n",
    "            topk_v, topk_i = output.topk(1, dim=2)\n",
    "            topk = torch.cat((topk,topk_i), dim=0)\n",
    "        loss_sum += loss/x.size(0)\n",
    "        total_loss += 1\n",
    "    #calculate metrics\n",
    "    final_loss = loss_sum / total_loss\n",
    "    perplexity = torch.exp(final_loss)\n",
    "    \n",
    "    #detokenize some sentence\n",
    "    batch_example = random.randint(0,x.size(1)-1)\n",
    "    input_sentence = detokenize(x.transpose(0,1)[0,:])\n",
    "    output_sentence = detokenize(topk.transpose(0,1)[0, :])\n",
    "    \n",
    "    #print everything\n",
    "    validation_info = \"\"\"-----------------------------------------------------\n",
    "    Validation:\n",
    "    loss: %.4f, perplexity: %.4f\n",
    "    input sentence: %s\n",
    "    output sentence: %s\n",
    "    ------------------------------------------------\n",
    "    \"\"\" % (final_loss, perplexity, input_sentence, output_sentence)\n",
    "    print(validation_info)\n",
    "    return final_loss, perplexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, criterion, optimizer, inputs):\n",
    "    x, target = inputs\n",
    "    x, target = x.to(device), target.to(device)\n",
    "    #loss_lengths = lengths.to(device)\n",
    "    hidden = None\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    output = x[0:1,:]\n",
    "    for t in range(0, x.size(0)):\n",
    "        output, hidden = model(x[t,:].unsqueeze(0),\n",
    "                               hidden)\n",
    "        l = criterion(output.squeeze(0), target[t, :])\n",
    "        loss += l\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss/x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, loader, epochs, print_every=10, save_every=1000):\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for i, example in enumerate(loader):\n",
    "            inputs = example['input_ids'].transpose(0,1)[:-1], example['input_ids'].transpose(0,1)[1:]\n",
    "            #lengths = example['lengths'].transpose(0,1)\n",
    "            output, loss = train_step(model, criterion, optimizer, inputs)\n",
    "            if i % print_every == 0:\n",
    "                print('epoch: %.d, iter: %.d, loss: %.4f' % \n",
    "                     (epoch, i, loss))\n",
    "            if i % save_every == 0:\n",
    "                torch.save({\n",
    "                    'iteration': i,\n",
    "                    'epoch': epoch,\n",
    "                    'model': model.state_dict()\n",
    "                }, './models/small-test/{}_{}.tar'.format(epoch,i))\n",
    "def one_batch(model, criterion, optimizer, batch, iters):\n",
    "    inputs = batch['input_ids'].transpose(0,1)[:-1], batch['input_ids'].transpose(0,1)[1:]\n",
    "    #lengths = batch['lengths'].transpose(0,1)\n",
    "    for i in range(iters):\n",
    "        output, loss = train_step(model, criterion, optimizer, inputs)\n",
    "        if i % 1 == 0:\n",
    "            print('iter: %.d, loss: %.4f' % \n",
    "                 (i, loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 0, loss: 3.4660\n",
      "epoch: 1, iter: 10, loss: 3.3504\n",
      "epoch: 1, iter: 20, loss: 3.3054\n",
      "epoch: 1, iter: 30, loss: 3.2551\n",
      "epoch: 1, iter: 40, loss: 3.2251\n",
      "epoch: 1, iter: 50, loss: 3.1872\n",
      "epoch: 1, iter: 60, loss: 3.2235\n",
      "epoch: 1, iter: 70, loss: 3.1042\n",
      "epoch: 1, iter: 80, loss: 3.1756\n",
      "epoch: 1, iter: 90, loss: 3.2290\n",
      "epoch: 1, iter: 100, loss: 3.2030\n",
      "epoch: 1, iter: 110, loss: 3.1262\n",
      "epoch: 1, iter: 120, loss: 3.1847\n",
      "epoch: 1, iter: 130, loss: 3.2515\n",
      "epoch: 1, iter: 140, loss: 3.1146\n",
      "epoch: 1, iter: 150, loss: 3.1223\n",
      "epoch: 1, iter: 160, loss: 3.1166\n",
      "epoch: 1, iter: 170, loss: 3.1083\n",
      "epoch: 1, iter: 180, loss: 3.0865\n",
      "epoch: 1, iter: 190, loss: 3.0430\n",
      "epoch: 1, iter: 200, loss: 3.1427\n",
      "epoch: 1, iter: 210, loss: 3.0463\n",
      "epoch: 1, iter: 220, loss: 3.1214\n",
      "epoch: 1, iter: 230, loss: 3.0082\n",
      "epoch: 1, iter: 240, loss: 2.9451\n",
      "epoch: 1, iter: 250, loss: 3.0754\n",
      "epoch: 1, iter: 260, loss: 3.0035\n",
      "epoch: 1, iter: 270, loss: 3.0120\n",
      "epoch: 1, iter: 280, loss: 2.9697\n",
      "epoch: 1, iter: 290, loss: 3.1607\n",
      "epoch: 1, iter: 300, loss: 3.0751\n",
      "epoch: 1, iter: 310, loss: 3.0442\n",
      "epoch: 1, iter: 320, loss: 2.9912\n",
      "epoch: 1, iter: 330, loss: 2.9965\n",
      "epoch: 1, iter: 340, loss: 3.1070\n",
      "epoch: 1, iter: 350, loss: 3.1246\n",
      "epoch: 1, iter: 360, loss: 3.0553\n",
      "epoch: 1, iter: 370, loss: 3.1433\n",
      "epoch: 1, iter: 380, loss: 3.0230\n",
      "epoch: 1, iter: 390, loss: 3.0270\n",
      "epoch: 1, iter: 400, loss: 3.0577\n",
      "epoch: 1, iter: 410, loss: 2.9773\n",
      "epoch: 1, iter: 420, loss: 2.8894\n",
      "epoch: 1, iter: 430, loss: 2.9931\n",
      "epoch: 1, iter: 440, loss: 3.0644\n",
      "epoch: 1, iter: 450, loss: 2.9901\n",
      "epoch: 1, iter: 460, loss: 3.0148\n",
      "epoch: 1, iter: 470, loss: 3.0516\n",
      "epoch: 1, iter: 480, loss: 2.9907\n",
      "epoch: 1, iter: 490, loss: 3.0751\n",
      "epoch: 1, iter: 500, loss: 3.0407\n",
      "epoch: 1, iter: 510, loss: 3.0226\n",
      "epoch: 1, iter: 520, loss: 2.9551\n",
      "epoch: 1, iter: 530, loss: 3.0861\n",
      "epoch: 1, iter: 540, loss: 2.9828\n",
      "epoch: 1, iter: 550, loss: 2.9879\n",
      "epoch: 1, iter: 560, loss: 3.0516\n",
      "epoch: 1, iter: 570, loss: 2.9292\n",
      "epoch: 1, iter: 580, loss: 2.9177\n",
      "epoch: 1, iter: 590, loss: 3.0510\n",
      "epoch: 1, iter: 600, loss: 3.0032\n",
      "epoch: 1, iter: 610, loss: 2.9825\n",
      "epoch: 1, iter: 620, loss: 3.1057\n",
      "epoch: 1, iter: 630, loss: 3.0849\n",
      "epoch: 1, iter: 640, loss: 2.9347\n",
      "epoch: 1, iter: 650, loss: 3.0050\n",
      "epoch: 1, iter: 660, loss: 2.9989\n",
      "epoch: 1, iter: 670, loss: 3.0645\n",
      "epoch: 1, iter: 680, loss: 2.9846\n",
      "epoch: 1, iter: 690, loss: 3.0025\n",
      "epoch: 1, iter: 700, loss: 3.0365\n",
      "epoch: 1, iter: 710, loss: 3.0015\n",
      "epoch: 1, iter: 720, loss: 2.8980\n",
      "epoch: 1, iter: 730, loss: 2.8824\n",
      "epoch: 1, iter: 740, loss: 3.0140\n",
      "epoch: 1, iter: 750, loss: 2.9118\n",
      "epoch: 1, iter: 760, loss: 3.0315\n",
      "epoch: 1, iter: 770, loss: 2.9763\n",
      "epoch: 1, iter: 780, loss: 2.9583\n",
      "epoch: 1, iter: 790, loss: 3.0456\n",
      "epoch: 1, iter: 800, loss: 2.8985\n",
      "epoch: 1, iter: 810, loss: 3.0664\n",
      "epoch: 1, iter: 820, loss: 3.0781\n",
      "epoch: 1, iter: 830, loss: 3.0415\n",
      "epoch: 1, iter: 840, loss: 2.9164\n",
      "epoch: 1, iter: 850, loss: 2.9776\n",
      "epoch: 1, iter: 860, loss: 2.9825\n",
      "epoch: 1, iter: 870, loss: 2.9460\n",
      "epoch: 1, iter: 880, loss: 2.9225\n",
      "epoch: 1, iter: 890, loss: 2.9891\n",
      "epoch: 1, iter: 900, loss: 2.9356\n",
      "epoch: 1, iter: 910, loss: 3.0944\n",
      "epoch: 1, iter: 920, loss: 2.8930\n",
      "epoch: 1, iter: 930, loss: 2.8394\n",
      "epoch: 1, iter: 940, loss: 2.9000\n",
      "epoch: 1, iter: 950, loss: 2.8737\n",
      "epoch: 1, iter: 960, loss: 2.9322\n",
      "epoch: 1, iter: 970, loss: 2.9488\n",
      "epoch: 1, iter: 980, loss: 2.9280\n",
      "epoch: 1, iter: 990, loss: 3.0600\n",
      "epoch: 1, iter: 1000, loss: 3.0033\n",
      "epoch: 1, iter: 1010, loss: 2.8895\n",
      "epoch: 1, iter: 1020, loss: 2.9256\n",
      "epoch: 1, iter: 1030, loss: 2.9456\n",
      "epoch: 1, iter: 1040, loss: 2.8672\n",
      "epoch: 1, iter: 1050, loss: 3.0183\n",
      "epoch: 1, iter: 1060, loss: 2.9590\n",
      "epoch: 1, iter: 1070, loss: 2.9285\n",
      "epoch: 1, iter: 1080, loss: 2.9165\n",
      "epoch: 1, iter: 1090, loss: 2.9450\n",
      "epoch: 1, iter: 1100, loss: 2.9796\n",
      "epoch: 1, iter: 1110, loss: 2.9128\n",
      "epoch: 1, iter: 1120, loss: 3.0601\n",
      "epoch: 1, iter: 1130, loss: 2.9513\n",
      "epoch: 1, iter: 1140, loss: 2.9781\n",
      "epoch: 1, iter: 1150, loss: 2.9341\n",
      "epoch: 1, iter: 1160, loss: 3.0134\n",
      "epoch: 1, iter: 1170, loss: 3.0181\n",
      "epoch: 1, iter: 1180, loss: 2.9721\n",
      "epoch: 1, iter: 1190, loss: 2.9956\n",
      "epoch: 1, iter: 1200, loss: 2.9196\n",
      "epoch: 1, iter: 1210, loss: 2.7736\n",
      "epoch: 1, iter: 1220, loss: 3.0487\n",
      "epoch: 1, iter: 1230, loss: 3.0703\n",
      "epoch: 1, iter: 1240, loss: 2.9186\n",
      "epoch: 1, iter: 1250, loss: 2.9676\n",
      "epoch: 1, iter: 1260, loss: 2.9497\n",
      "epoch: 1, iter: 1270, loss: 3.0169\n",
      "epoch: 1, iter: 1280, loss: 2.9492\n",
      "epoch: 1, iter: 1290, loss: 2.8673\n",
      "epoch: 1, iter: 1300, loss: 3.0027\n",
      "epoch: 1, iter: 1310, loss: 2.9070\n",
      "epoch: 1, iter: 1320, loss: 2.9562\n",
      "epoch: 1, iter: 1330, loss: 2.9562\n",
      "epoch: 1, iter: 1340, loss: 2.9973\n",
      "epoch: 1, iter: 1350, loss: 2.9671\n",
      "epoch: 1, iter: 1360, loss: 3.0074\n",
      "epoch: 1, iter: 1370, loss: 2.9047\n",
      "epoch: 1, iter: 1380, loss: 2.8992\n",
      "epoch: 1, iter: 1390, loss: 2.9278\n",
      "epoch: 1, iter: 1400, loss: 2.9001\n",
      "epoch: 1, iter: 1410, loss: 2.9217\n",
      "epoch: 1, iter: 1420, loss: 2.9738\n",
      "epoch: 1, iter: 1430, loss: 2.9720\n",
      "epoch: 1, iter: 1440, loss: 3.0047\n",
      "epoch: 1, iter: 1450, loss: 2.8355\n",
      "epoch: 1, iter: 1460, loss: 2.8545\n",
      "epoch: 1, iter: 1470, loss: 3.0409\n",
      "epoch: 1, iter: 1480, loss: 3.0563\n",
      "epoch: 1, iter: 1490, loss: 2.9406\n",
      "epoch: 1, iter: 1500, loss: 3.0192\n",
      "epoch: 1, iter: 1510, loss: 2.9946\n",
      "epoch: 1, iter: 1520, loss: 2.9045\n",
      "epoch: 1, iter: 1530, loss: 2.9064\n",
      "epoch: 1, iter: 1540, loss: 2.9825\n",
      "epoch: 1, iter: 1550, loss: 2.9515\n",
      "epoch: 1, iter: 1560, loss: 3.0157\n",
      "epoch: 1, iter: 1570, loss: 2.9065\n",
      "epoch: 1, iter: 1580, loss: 3.0509\n",
      "epoch: 1, iter: 1590, loss: 2.7577\n",
      "epoch: 1, iter: 1600, loss: 2.9922\n",
      "epoch: 1, iter: 1610, loss: 2.9320\n",
      "epoch: 1, iter: 1620, loss: 3.0691\n",
      "epoch: 1, iter: 1630, loss: 2.9256\n",
      "epoch: 1, iter: 1640, loss: 2.9581\n",
      "epoch: 1, iter: 1650, loss: 2.9603\n",
      "epoch: 1, iter: 1660, loss: 3.0185\n",
      "epoch: 1, iter: 1670, loss: 2.9989\n",
      "epoch: 1, iter: 1680, loss: 2.9570\n",
      "epoch: 1, iter: 1690, loss: 3.0137\n",
      "epoch: 1, iter: 1700, loss: 2.9676\n",
      "epoch: 1, iter: 1710, loss: 3.0700\n",
      "epoch: 1, iter: 1720, loss: 3.0083\n",
      "epoch: 1, iter: 1730, loss: 2.9925\n",
      "epoch: 1, iter: 1740, loss: 2.9721\n",
      "epoch: 1, iter: 1750, loss: 3.0305\n",
      "epoch: 1, iter: 1760, loss: 2.9071\n",
      "epoch: 1, iter: 1770, loss: 2.9766\n",
      "epoch: 1, iter: 1780, loss: 2.9543\n",
      "epoch: 1, iter: 1790, loss: 3.0397\n",
      "epoch: 1, iter: 1800, loss: 2.9145\n",
      "epoch: 1, iter: 1810, loss: 2.8769\n",
      "epoch: 1, iter: 1820, loss: 2.8855\n",
      "epoch: 1, iter: 1830, loss: 2.9322\n",
      "epoch: 1, iter: 1840, loss: 2.9057\n",
      "epoch: 1, iter: 1850, loss: 2.9251\n",
      "epoch: 1, iter: 1860, loss: 2.9221\n",
      "epoch: 1, iter: 1870, loss: 2.9707\n",
      "epoch: 1, iter: 1880, loss: 2.9446\n",
      "epoch: 1, iter: 1890, loss: 2.9504\n",
      "epoch: 1, iter: 1900, loss: 2.9869\n",
      "epoch: 1, iter: 1910, loss: 2.8387\n",
      "epoch: 1, iter: 1920, loss: 2.9901\n",
      "epoch: 1, iter: 1930, loss: 2.8303\n",
      "epoch: 1, iter: 1940, loss: 3.0797\n",
      "epoch: 1, iter: 1950, loss: 3.0015\n",
      "epoch: 1, iter: 1960, loss: 2.9658\n",
      "epoch: 1, iter: 1970, loss: 2.9667\n",
      "epoch: 1, iter: 1980, loss: 2.9496\n",
      "epoch: 1, iter: 1990, loss: 2.9851\n",
      "epoch: 1, iter: 2000, loss: 2.8741\n",
      "epoch: 1, iter: 2010, loss: 2.9508\n",
      "epoch: 1, iter: 2020, loss: 2.9679\n",
      "epoch: 1, iter: 2030, loss: 2.9815\n",
      "epoch: 1, iter: 2040, loss: 2.8929\n",
      "epoch: 1, iter: 2050, loss: 2.9620\n",
      "epoch: 1, iter: 2060, loss: 2.8468\n",
      "epoch: 1, iter: 2070, loss: 2.9551\n",
      "epoch: 1, iter: 2080, loss: 2.9444\n",
      "epoch: 1, iter: 2090, loss: 2.9017\n",
      "epoch: 1, iter: 2100, loss: 2.9734\n",
      "epoch: 1, iter: 2110, loss: 2.9260\n",
      "epoch: 1, iter: 2120, loss: 3.0323\n",
      "epoch: 1, iter: 2130, loss: 2.9126\n",
      "epoch: 1, iter: 2140, loss: 2.9796\n",
      "epoch: 1, iter: 2150, loss: 2.9936\n",
      "epoch: 1, iter: 2160, loss: 2.9927\n",
      "epoch: 1, iter: 2170, loss: 2.9445\n",
      "epoch: 1, iter: 2180, loss: 2.9439\n",
      "epoch: 1, iter: 2190, loss: 2.8736\n",
      "epoch: 1, iter: 2200, loss: 2.9810\n",
      "epoch: 1, iter: 2210, loss: 2.9615\n",
      "epoch: 1, iter: 2220, loss: 2.8940\n",
      "epoch: 1, iter: 2230, loss: 2.8320\n",
      "epoch: 1, iter: 2240, loss: 2.9231\n",
      "epoch: 1, iter: 2250, loss: 2.8540\n",
      "epoch: 1, iter: 2260, loss: 2.9171\n",
      "epoch: 1, iter: 2270, loss: 3.0120\n",
      "epoch: 1, iter: 2280, loss: 2.9311\n",
      "epoch: 1, iter: 2290, loss: 2.7968\n",
      "epoch: 1, iter: 2300, loss: 2.9214\n",
      "epoch: 1, iter: 2310, loss: 2.9382\n",
      "epoch: 1, iter: 2320, loss: 2.9456\n",
      "epoch: 1, iter: 2330, loss: 2.9993\n",
      "epoch: 1, iter: 2340, loss: 2.8577\n",
      "epoch: 1, iter: 2350, loss: 2.9777\n",
      "epoch: 1, iter: 2360, loss: 2.9551\n",
      "epoch: 1, iter: 2370, loss: 2.9561\n",
      "epoch: 1, iter: 2380, loss: 3.0228\n",
      "epoch: 1, iter: 2390, loss: 3.0094\n",
      "epoch: 1, iter: 2400, loss: 2.9643\n",
      "epoch: 1, iter: 2410, loss: 2.9471\n",
      "epoch: 1, iter: 2420, loss: 2.9980\n",
      "epoch: 1, iter: 2430, loss: 2.9339\n",
      "epoch: 1, iter: 2440, loss: 2.9202\n",
      "epoch: 1, iter: 2450, loss: 2.8583\n",
      "epoch: 1, iter: 2460, loss: 3.1108\n",
      "epoch: 1, iter: 2470, loss: 2.7876\n",
      "epoch: 1, iter: 2480, loss: 2.9167\n",
      "epoch: 1, iter: 2490, loss: 2.9678\n",
      "epoch: 1, iter: 2500, loss: 2.9038\n",
      "epoch: 1, iter: 2510, loss: 2.9933\n",
      "epoch: 1, iter: 2520, loss: 2.9663\n",
      "epoch: 1, iter: 2530, loss: 2.8777\n",
      "epoch: 1, iter: 2540, loss: 2.8891\n",
      "epoch: 1, iter: 2550, loss: 2.8649\n",
      "epoch: 1, iter: 2560, loss: 2.8560\n",
      "epoch: 1, iter: 2570, loss: 2.8718\n",
      "epoch: 1, iter: 2580, loss: 2.9905\n",
      "epoch: 1, iter: 2590, loss: 3.0507\n",
      "epoch: 1, iter: 2600, loss: 2.8780\n",
      "epoch: 1, iter: 2610, loss: 2.9876\n",
      "epoch: 1, iter: 2620, loss: 2.9221\n",
      "epoch: 1, iter: 2630, loss: 2.8806\n",
      "epoch: 1, iter: 2640, loss: 2.9046\n",
      "epoch: 1, iter: 2650, loss: 2.9069\n",
      "epoch: 1, iter: 2660, loss: 2.9929\n",
      "epoch: 1, iter: 2670, loss: 2.9218\n",
      "epoch: 1, iter: 2680, loss: 2.8047\n",
      "epoch: 1, iter: 2690, loss: 2.9566\n",
      "epoch: 1, iter: 2700, loss: 3.0171\n",
      "epoch: 1, iter: 2710, loss: 2.8683\n",
      "epoch: 1, iter: 2720, loss: 2.9455\n",
      "epoch: 1, iter: 2730, loss: 2.9168\n",
      "epoch: 1, iter: 2740, loss: 2.9174\n",
      "epoch: 1, iter: 2750, loss: 2.9004\n",
      "epoch: 1, iter: 2760, loss: 3.0019\n",
      "epoch: 1, iter: 2770, loss: 3.0100\n",
      "epoch: 1, iter: 2780, loss: 3.0162\n",
      "epoch: 1, iter: 2790, loss: 2.9331\n",
      "epoch: 1, iter: 2800, loss: 2.9987\n",
      "epoch: 1, iter: 2810, loss: 2.9301\n",
      "epoch: 1, iter: 2820, loss: 2.8780\n",
      "epoch: 1, iter: 2830, loss: 2.8736\n",
      "epoch: 1, iter: 2840, loss: 3.0022\n",
      "epoch: 1, iter: 2850, loss: 2.9923\n",
      "epoch: 2, iter: 0, loss: 3.0141\n",
      "epoch: 2, iter: 10, loss: 2.8633\n",
      "epoch: 2, iter: 20, loss: 3.0328\n",
      "epoch: 2, iter: 30, loss: 2.9112\n",
      "epoch: 2, iter: 40, loss: 2.8851\n",
      "epoch: 2, iter: 50, loss: 2.9399\n",
      "epoch: 2, iter: 60, loss: 3.0077\n",
      "epoch: 2, iter: 70, loss: 2.8372\n",
      "epoch: 2, iter: 80, loss: 2.9529\n",
      "epoch: 2, iter: 90, loss: 2.8392\n",
      "epoch: 2, iter: 100, loss: 2.7820\n",
      "epoch: 2, iter: 110, loss: 2.9553\n",
      "epoch: 2, iter: 120, loss: 2.9474\n",
      "epoch: 2, iter: 130, loss: 2.8736\n",
      "epoch: 2, iter: 140, loss: 2.9700\n",
      "epoch: 2, iter: 150, loss: 2.9735\n",
      "epoch: 2, iter: 160, loss: 2.8590\n",
      "epoch: 2, iter: 170, loss: 2.8844\n",
      "epoch: 2, iter: 180, loss: 2.9261\n",
      "epoch: 2, iter: 190, loss: 2.7907\n",
      "epoch: 2, iter: 200, loss: 2.9938\n",
      "epoch: 2, iter: 210, loss: 2.9369\n",
      "epoch: 2, iter: 220, loss: 2.9447\n",
      "epoch: 2, iter: 230, loss: 2.9045\n",
      "epoch: 2, iter: 240, loss: 2.8781\n",
      "epoch: 2, iter: 250, loss: 2.8929\n",
      "epoch: 2, iter: 260, loss: 3.0624\n",
      "epoch: 2, iter: 270, loss: 2.8554\n",
      "epoch: 2, iter: 280, loss: 3.0511\n",
      "epoch: 2, iter: 290, loss: 2.8069\n",
      "epoch: 2, iter: 300, loss: 3.0280\n",
      "epoch: 2, iter: 310, loss: 2.7566\n",
      "epoch: 2, iter: 320, loss: 2.9645\n",
      "epoch: 2, iter: 330, loss: 2.8649\n",
      "epoch: 2, iter: 340, loss: 3.0013\n",
      "epoch: 2, iter: 350, loss: 2.8874\n",
      "epoch: 2, iter: 360, loss: 2.9412\n",
      "epoch: 2, iter: 370, loss: 2.9494\n",
      "epoch: 2, iter: 380, loss: 3.0869\n",
      "epoch: 2, iter: 390, loss: 2.8749\n",
      "epoch: 2, iter: 400, loss: 2.8866\n",
      "epoch: 2, iter: 410, loss: 2.9760\n",
      "epoch: 2, iter: 420, loss: 2.8994\n",
      "epoch: 2, iter: 430, loss: 2.9156\n",
      "epoch: 2, iter: 440, loss: 2.9609\n",
      "epoch: 2, iter: 450, loss: 3.0460\n",
      "epoch: 2, iter: 460, loss: 2.8727\n",
      "epoch: 2, iter: 470, loss: 2.9818\n",
      "epoch: 2, iter: 480, loss: 2.9770\n",
      "epoch: 2, iter: 490, loss: 2.8946\n",
      "epoch: 2, iter: 500, loss: 2.9139\n",
      "epoch: 2, iter: 510, loss: 3.0096\n",
      "epoch: 2, iter: 520, loss: 2.8067\n",
      "epoch: 2, iter: 530, loss: 2.9482\n",
      "epoch: 2, iter: 540, loss: 2.9838\n",
      "epoch: 2, iter: 550, loss: 2.9252\n",
      "epoch: 2, iter: 560, loss: 2.9089\n",
      "epoch: 2, iter: 570, loss: 2.8281\n",
      "epoch: 2, iter: 580, loss: 3.0095\n",
      "epoch: 2, iter: 590, loss: 2.8355\n",
      "epoch: 2, iter: 600, loss: 2.9327\n",
      "epoch: 2, iter: 610, loss: 2.9196\n",
      "epoch: 2, iter: 620, loss: 2.8496\n",
      "epoch: 2, iter: 630, loss: 2.8505\n",
      "epoch: 2, iter: 640, loss: 2.9501\n",
      "epoch: 2, iter: 650, loss: 2.9511\n",
      "epoch: 2, iter: 660, loss: 2.8678\n",
      "epoch: 2, iter: 670, loss: 2.7535\n",
      "epoch: 2, iter: 680, loss: 2.9723\n",
      "epoch: 2, iter: 690, loss: 2.8516\n",
      "epoch: 2, iter: 700, loss: 2.9114\n",
      "epoch: 2, iter: 710, loss: 2.9481\n",
      "epoch: 2, iter: 720, loss: 2.8145\n",
      "epoch: 2, iter: 730, loss: 2.8557\n",
      "epoch: 2, iter: 740, loss: 2.9834\n",
      "epoch: 2, iter: 750, loss: 2.8878\n",
      "epoch: 2, iter: 760, loss: 2.7454\n",
      "epoch: 2, iter: 770, loss: 2.9337\n",
      "epoch: 2, iter: 780, loss: 2.9339\n",
      "epoch: 2, iter: 790, loss: 2.9140\n",
      "epoch: 2, iter: 800, loss: 2.8347\n",
      "epoch: 2, iter: 810, loss: 2.8255\n",
      "epoch: 2, iter: 820, loss: 2.8549\n",
      "epoch: 2, iter: 830, loss: 2.8615\n",
      "epoch: 2, iter: 840, loss: 2.9644\n",
      "epoch: 2, iter: 850, loss: 2.9574\n",
      "epoch: 2, iter: 860, loss: 2.8589\n",
      "epoch: 2, iter: 870, loss: 2.9851\n",
      "epoch: 2, iter: 880, loss: 2.9300\n",
      "epoch: 2, iter: 890, loss: 2.7916\n",
      "epoch: 2, iter: 900, loss: 2.8632\n",
      "epoch: 2, iter: 910, loss: 2.8710\n",
      "epoch: 2, iter: 920, loss: 2.9486\n",
      "epoch: 2, iter: 930, loss: 2.9106\n",
      "epoch: 2, iter: 940, loss: 2.8652\n",
      "epoch: 2, iter: 950, loss: 2.8820\n",
      "epoch: 2, iter: 960, loss: 2.9565\n",
      "epoch: 2, iter: 970, loss: 2.9401\n",
      "epoch: 2, iter: 980, loss: 3.0502\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "model = LMGRU(vocab_size, 128, 64, 8, 0.1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def maskNLLLoss(output, target, lengths):\n",
    "    print(lengths)\n",
    "    print(target)\n",
    "    total = lengths.sum()\n",
    "    cross_entropy = -torch.log(torch.gather(output, 1, target.view(-1,1)).squeeze(1))\n",
    "    print(cross_entropy)\n",
    "    loss = (cross_entropy*lengths).sum() / total\n",
    "    loss = loss.to(device)\n",
    "    return loss\n",
    "\n",
    "#one_batch(model, criterion, optimizer, batch, 500) # overfit one batch\n",
    "train(model, criterion, optimizer, dataloader, 2, 10) # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd17b62372548d2ac86529ef386f656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821ac886d92746e1bd2ca50283516ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1382015.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlnet-large-cased', keep_accents=True, bos_token='<s>', eos_token='</s>',unk_token='<u>', pad_token='<p>', padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933704ffdc9242b3a136de95791942a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=911021.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78aa6a1ce8f647e389281df3c45626aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=901.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(e):\n",
    "    return tokenizer(e['text'], add_special_tokens=True,  padding='longest')\n",
    "dataset = dataset.map(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'input_ids': [2073,\n",
       "  1182,\n",
       "  1284,\n",
       "  46,\n",
       "  12806,\n",
       "  22184,\n",
       "  2483,\n",
       "  17,\n",
       "  6884,\n",
       "  556,\n",
       "  3879,\n",
       "  17,\n",
       "  12,\n",
       "  874,\n",
       "  4287,\n",
       "  12,\n",
       "  19,\n",
       "  17,\n",
       "  7522,\n",
       "  4304,\n",
       "  17,\n",
       "  254,\n",
       "  2483,\n",
       "  2605,\n",
       "  101,\n",
       "  321,\n",
       "  13953,\n",
       "  3423,\n",
       "  202,\n",
       "  17,\n",
       "  117,\n",
       "  4425,\n",
       "  13792,\n",
       "  5074,\n",
       "  11760,\n",
       "  772,\n",
       "  5894,\n",
       "  1868,\n",
       "  150,\n",
       "  9,\n",
       "  4,\n",
       "  3],\n",
       " 'text': 'En 2011 protagonizó la película \"Hick\", basada en la novela de Andrea Portes y dirigida por Derick Martini.',\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1]['input_ids'].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
